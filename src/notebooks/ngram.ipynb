{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "from typing import Union"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import string\n",
    "NUM_CHARS = 27\n",
    "CHARS = list(string.ascii_lowercase)\n",
    "CTOI = {c: i+1 for i, c in enumerate(CHARS)}\n",
    "CTOI['.'] = 0\n",
    "ITOC = {i: c for c, i in CTOI.items()}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "g = torch.Generator().manual_seed(2147483647)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Dataset:\n",
    "\n",
    "  def __init__(self, path: str, split: list = [0.8, 0.1, 0.1]) -> None:\n",
    "    self.words = open(path, 'r').read().splitlines()\n",
    "    self.train_set, test_eval = train_test_split(self.words, train_size=split[0])\n",
    "    self.eval_set, self.test_set = train_test_split(test_eval, train_size=split[1])\n",
    "\n",
    "data = Dataset('../../data/names.txt')\n",
    "print(len(data.train_set))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [],
   "source": [
    "class NGramExplicitModel:\n",
    "\n",
    "  def __init__(self, n: int, data: Dataset) -> None:\n",
    "    self.n = n\n",
    "    self.data = data\n",
    "\n",
    "  def count(self):\n",
    "    self.counts = torch.zeros(tuple([NUM_CHARS for _ in range(self.n)]), dtype=torch.int32)\n",
    "    for word in self.data.train_set:\n",
    "      context = [0] * (self.n-1)\n",
    "      for c in word + '.':\n",
    "        ix = CTOI[c]\n",
    "        indices = tuple(context + [ix])\n",
    "        self.counts[indices] += 1\n",
    "        context = context[1:] + [ix]\n",
    "      \n",
    "    self.P = F.normalize((self.counts+1).float(), p=1, dim=-1)\n",
    "  \n",
    "  def sample(self, generator: torch.Generator, num_samples: int = 1):\n",
    "    for i in range(num_samples):\n",
    "      sample = []\n",
    "      context = [0 for _ in range(self.n-1)]\n",
    "      while True:\n",
    "        ix = torch.multinomial(self.P[tuple(context)], num_samples=1, replacement=True, generator=generator).item()\n",
    "        sample.append(ITOC[ix])\n",
    "        if ix == 0:\n",
    "          break\n",
    "        context = context[1:] + [ix]\n",
    "\n",
    "      print(''.join(sample))\n",
    "\n",
    "  def eval(self, eval_set: Union[str, Dataset] = 'eval'):\n",
    "    # average neg log likelihood\n",
    "    llh, n = 0, 0\n",
    "    if type(eval_set) is str:\n",
    "      eval_set = self.data.eval_set if eval_set=='eval' else self.data.train_set if eval_set=='train' else self.data.test_set\n",
    "    else:\n",
    "      eval_set = eval_set.eval_set\n",
    "    for word in eval_set:\n",
    "      context = [0 for _ in range(self.n-1)]\n",
    "      for c in word + '.':\n",
    "        ix = CTOI[c]\n",
    "        indices = tuple(context + [ix])\n",
    "        logprob = torch.log(self.P[indices])\n",
    "        llh += logprob\n",
    "        n += 1\n",
    "        context = context[1:] + [ix]\n",
    "    return -llh/n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Linear(nn.Module):\n",
    "\n",
    "  def __init__(self, n_in: int, n_out: int, bias: bool) -> None:\n",
    "    super().__init__()\n",
    "    self.W = torch.randn((n_in, n_out), generator=g) / (n_in)**0.5\n",
    "    self.b = torch.randn(n_out, generator=g) if bias else None\n",
    "  \n",
    "  def forward(self, x):\n",
    "    self.out = x @ self.W \n",
    "    if self.b is not None:\n",
    "      self.out += self.b\n",
    "    return self.out\n",
    "\n",
    "  def parameters(self):\n",
    "    return [self.W] + ([self.b] if self.b is not None else [])\n",
    "\n",
    "\n",
    "class Tanh(nn.Module):\n",
    "\n",
    "  def __init__(self) -> None:\n",
    "    super().__init__()\n",
    "  \n",
    "  def forward(self, x):\n",
    "    self.out = torch.tanh(x)\n",
    "    return self.out\n",
    "\n",
    "  def parameters(self):\n",
    "    return []\n",
    "  \n",
    "\n",
    "class BatchNorm1d(nn.Module):\n",
    "\n",
    "  def __init__(self, dim: int, eps: float = 1e-4, momentum: float = 0.1) -> None:\n",
    "    super().__init__()\n",
    "    self.training = True\n",
    "    self.running_mean = torch.zeros(dim)\n",
    "    self.running_var = torch.ones(dim)\n",
    "    self.eps = eps\n",
    "    self.gamma = torch.ones(dim)\n",
    "    self.beta = torch.zeros(dim)\n",
    "    self.momentum = momentum\n",
    "\n",
    "  def forward(self, x: torch.Tensor):\n",
    "    if self.training:\n",
    "      x_mean = x.mean(0, keepdim=True)\n",
    "      x_var = x.var(0, keepdim=True)\n",
    "    else:\n",
    "      x_mean = self.running_mean\n",
    "      x_var = self.running_var\n",
    "\n",
    "    x_hat = (x - x_mean) / torch.sqrt(x_var + self.eps)\n",
    "    self.out = self.gamma * x_hat + self.beta\n",
    "\n",
    "    if self.training:\n",
    "      with torch.no_grad():\n",
    "        self.running_mean = (1-self.momentum) * self.running_mean + self.momentum * x_mean\n",
    "        self.running_var = (1-self.momentum) * self.running_var + self.momentum * x_var\n",
    "    return self.out\n",
    "  \n",
    "  def parameters(self):\n",
    "    return [self.gamma, self.beta]\n",
    "\n",
    "\n",
    "class Embedding(nn.Module):\n",
    "\n",
    "  def __init__(self, embed_dim: int, context_len: int) -> None:\n",
    "    super().__init__()\n",
    "    self.embed_dim = embed_dim\n",
    "    self.context_len = context_len\n",
    "    self.C = torch.randn((NUM_CHARS, embed_dim), generator=g)\n",
    "  \n",
    "  def forward(self, x):\n",
    "    self.out = self.C[x]\n",
    "    self.out = self.out.view(-1, (self.context_len)*self.embed_dim)\n",
    "    return self.out\n",
    "  \n",
    "  def parameters(self):\n",
    "    return [self.C]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MLP(nn.Module):\n",
    "\n",
    "  def __init__(self, context_len: int, embed_dim: int, hidden_layer_size: int) -> None:\n",
    "    super().__init__()\n",
    "    self.context_len = context_len\n",
    "    self.embed_dim = embed_dim\n",
    "    self.layers = [\n",
    "      Embedding(embed_dim, context_len), \n",
    "      Linear(context_len*embed_dim, hidden_layer_size, bias=False),\n",
    "      BatchNorm1d(hidden_layer_size), Tanh(),\n",
    "      Linear(hidden_layer_size, hidden_layer_size, bias=False),\n",
    "      BatchNorm1d(hidden_layer_size), Tanh(),\n",
    "      Linear(hidden_layer_size, NUM_CHARS, bias=False),\n",
    "      BatchNorm1d(NUM_CHARS)\n",
    "    ]\n",
    "    self.params = [p for l in self.layers for p in l.parameters()]\n",
    "    for p in self.params:\n",
    "      p.requires_grad = True\n",
    "  \n",
    "  def forward(self, x):\n",
    "    self.out = x\n",
    "    for l in self.layers:\n",
    "      self.out = l(self.out)\n",
    "    return self.out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {},
   "outputs": [],
   "source": [
    "class NGramMLPModel:\n",
    "\n",
    "  def __init__(self, n: int, data: Dataset, embed_dim: int, rc: float, hidden_layer_size: int = 100, generator: torch.Generator = None) -> None:\n",
    "    self.n = n\n",
    "    self.data = data\n",
    "    self.rc = rc          # regularization coefficient\n",
    "    self.xs, self.ys = self.prepare_ds()\n",
    "    # self.net = TwoLayerMLP(self.n-1, embed_dim, hidden_layer_size, generator)\n",
    "    self.net = MLP(self.n-1, embed_dim, hidden_layer_size)\n",
    "\n",
    "  def prepare_ds(self, prepare_set: Union[str, Dataset] = 'train'):\n",
    "    if type(prepare_set) is str:\n",
    "      prepare_set = self.data.eval_set if prepare_set=='eval' else self.data.train_set if prepare_set=='train' else self.data.test_set\n",
    "    else: \n",
    "      prepare_set = prepare_set.train_set\n",
    "    # Create train set\n",
    "    xs, ys = [], []\n",
    "    for w in prepare_set:\n",
    "      context = [0] * (self.n-1) \n",
    "      for c in w + '.':\n",
    "        ix = CTOI[c]\n",
    "        xs.append(context)\n",
    "        ys.append(CTOI[c])\n",
    "        context = context[1:] + [ix]\n",
    "    xs = torch.tensor(xs)\n",
    "    ys = torch.tensor(ys)\n",
    "    return xs, ys\n",
    "\n",
    "  def compute_loss(self, logits, y):\n",
    "    return F.cross_entropy(logits, y) # + self.rc*(self.net.W1**2).mean() + self.rc*(self.net.W2**2).mean()\n",
    "\n",
    "\n",
    "  def train_network(self, num_iters: int, lr: float, batch_size: int, generator: torch.Generator):\n",
    "    self.train_mode()\n",
    "    losses = []\n",
    "    for k in range(num_iters):\n",
    "      # minibatch construct\n",
    "      ix = torch.randint(0, self.xs.shape[0], (batch_size,), generator=generator)\n",
    "      Xb, Yb = self.xs[ix], self.ys[ix] # batch X,Y\n",
    "\n",
    "      # NN forward pass\n",
    "      logits = self.net(self.xs)\n",
    "\n",
    "      # loss: negative llh of probs corresponding to true labels\n",
    "      loss = self.compute_loss(logits, self.ys)\n",
    "      \n",
    "      ## NN backward pass\n",
    "      for p in self.net.params:\n",
    "        p.grad = None       # set grad to 0\n",
    "      loss.backward()\n",
    "      if k%10 == 0:\n",
    "        print(f'Iter {k}, loss {loss.item()}')\n",
    "      losses.append(loss.item())\n",
    "      for p in self.net.params:\n",
    "        p.data += -lr*p.grad\n",
    "    \n",
    "    # plot losses\n",
    "    plt.plot(range(len(losses)), losses)\n",
    "    plt.ylim(bottom=0)\n",
    "    plt.xlabel('Training epoch'); plt.ylabel('Loss')\n",
    "    plt.show()\n",
    "\n",
    "\n",
    "  def sample(self, generator: torch.Generator, num_samples: int):\n",
    "    self.eval_mode()\n",
    "    for i in range(num_samples):\n",
    "      sample = []\n",
    "      context = [0 for _ in range(self.n-1)]\n",
    "      while True:\n",
    "        logits = self.net(torch.tensor(context))\n",
    "        counts = logits.exp()\n",
    "        p = F.normalize(counts.float(), p=1, dim=-1)\n",
    "\n",
    "        # print(p)\n",
    "\n",
    "        ix = torch.multinomial(p, num_samples=1, replacement=True, generator=generator).item()\n",
    "        sample.append(ITOC[ix])\n",
    "        if ix == 0:\n",
    "          break\n",
    "        context = context[1:] + [ix]\n",
    "\n",
    "      print(''.join(sample))\n",
    "\n",
    "  def eval(self, eval_set: Union[str, Dataset] = 'eval'):\n",
    "    # average neg log likelihood\n",
    "    self.eval_mode()\n",
    "    xs, ys = self.prepare_ds(eval_set)\n",
    "    logits = self.net(xs)\n",
    "    loss = self.compute_loss(logits, ys)   \n",
    "    return loss.item()\n",
    "  \n",
    "  def eval_mode(self):\n",
    "    for layer in self.net.layers:\n",
    "      layer.training = False\n",
    "  \n",
    "  def train_mode(self):\n",
    "    for layer in self.net.layers:\n",
    "      layer.training = True\n",
    "  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {},
   "outputs": [],
   "source": [
    "g = torch.Generator().manual_seed(2147483647)\n",
    "data = Dataset('../../data/names.txt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(2.4582)"
      ]
     },
     "execution_count": 87,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model = NGramExplicitModel(n=2, data=data)\n",
    "model.count()\n",
    "model.eval('eval')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iter 0, loss 3.7902445793151855\n",
      "Iter 10, loss 2.6899914741516113\n"
     ]
    }
   ],
   "source": [
    "%matplotlib inline\n",
    "g = torch.Generator().manual_seed(2147483647)\n",
    "for rc in [0.01]:\n",
    "  model = NGramMLPModel(2, data, 20, rc, 100, g)\n",
    "  model.train_network(50, 1.0, 32, g)\n",
    "  # model.train_network(120, 0.1, 32, g)\n",
    "  loss = model.eval('eval')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "g = torch.Generator().manual_seed(2147483647)\n",
    "model.sample(generator=g, num_samples=5)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "isp",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "21cbe5e4507590016ddf6079d506494e7f2eacb4fe848e3412c5364edd85520f"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
