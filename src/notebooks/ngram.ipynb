{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 137,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import torch\n",
    "import torch.nn.functional as F\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "from typing import Union"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 138,
   "metadata": {},
   "outputs": [],
   "source": [
    "import string\n",
    "NUM_CHARS = 27\n",
    "CHARS = list(string.ascii_lowercase)\n",
    "CTOI = {c: i+1 for i, c in enumerate(CHARS)}\n",
    "CTOI['.'] = 0\n",
    "ITOC = {i: c for c, i in CTOI.items()}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 139,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "25626\n"
     ]
    }
   ],
   "source": [
    "class Dataset:\n",
    "\n",
    "  def __init__(self, path: str, split: list = [0.8, 0.1, 0.1]) -> None:\n",
    "    self.words = open(path, 'r').read().splitlines()\n",
    "    self.train_set, test_eval = train_test_split(self.words, train_size=split[0])\n",
    "    self.eval_set, self.test_set = train_test_split(test_eval, train_size=split[1])\n",
    "\n",
    "data = Dataset('../../data/names.txt')\n",
    "print(len(data.train_set))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 140,
   "metadata": {},
   "outputs": [],
   "source": [
    "class NGramExplicitModel:\n",
    "\n",
    "  def __init__(self, n: int, data: Dataset) -> None:\n",
    "    self.n = n\n",
    "    self.data = data\n",
    "\n",
    "  def count(self):\n",
    "    self.counts = torch.zeros(tuple([NUM_CHARS for _ in range(self.n)]), dtype=torch.int32)\n",
    "    for word in self.data.train_set:\n",
    "      context = [0] * (self.n-1)\n",
    "      for c in word + '.':\n",
    "        ix = CTOI[c]\n",
    "        indices = tuple(context + [ix])\n",
    "        self.counts[indices] += 1\n",
    "        context = context[1:] + [ix]\n",
    "      \n",
    "    self.P = F.normalize((self.counts+1).float(), p=1, dim=-1)\n",
    "  \n",
    "  def sample(self, generator: torch.Generator, num_samples: int = 1):\n",
    "    for i in range(num_samples):\n",
    "      sample = []\n",
    "      context = [0 for _ in range(self.n-1)]\n",
    "      while True:\n",
    "        ix = torch.multinomial(self.P[tuple(context)], num_samples=1, replacement=True, generator=generator).item()\n",
    "        sample.append(ITOC[ix])\n",
    "        if ix == 0:\n",
    "          break\n",
    "        context = context[1:] + [ix]\n",
    "\n",
    "      print(''.join(sample))\n",
    "\n",
    "  def eval(self, eval_set: Union[str, Dataset] = 'eval'):\n",
    "    # average neg log likelihood\n",
    "    llh, n = 0, 0\n",
    "    if type(eval_set) is str:\n",
    "      eval_set = self.data.eval_set if eval_set=='eval' else self.data.train_set if eval_set=='train' else self.data.test_set\n",
    "    else:\n",
    "      eval_set = eval_set.eval_set\n",
    "    for word in eval_set:\n",
    "      context = [0 for _ in range(self.n-1)]\n",
    "      for c in word + '.':\n",
    "        ix = CTOI[c]\n",
    "        indices = tuple(context + [ix])\n",
    "        logprob = torch.log(self.P[indices])\n",
    "        llh += logprob\n",
    "        n += 1\n",
    "        context = context[1:] + [ix]\n",
    "    return -llh/n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 141,
   "metadata": {},
   "outputs": [],
   "source": [
    "class NGramMLPModel:\n",
    "\n",
    "  def __init__(self, n: int, data: Dataset, embed_dim: int, rc: float) -> None:\n",
    "    self.n = n\n",
    "    self.data = data\n",
    "    self.embed_dim = embed_dim\n",
    "    self.rc = rc          # regularization coefficient\n",
    "    self.xs, self.ys = self.prepare_ds()\n",
    "    self.init_network()\n",
    "\n",
    "  def prepare_ds(self, prepare_set: Union[str, Dataset] = 'train'):\n",
    "    if type(prepare_set) is str:\n",
    "      prepare_set = self.data.eval_set if prepare_set=='eval' else self.data.train_set if prepare_set=='train' else self.data.test_set\n",
    "    else: \n",
    "      prepare_set = prepare_set.train_set\n",
    "    # Create train set\n",
    "    xs, ys = [], []\n",
    "    for w in prepare_set:\n",
    "      context = [0] * (self.n-1) \n",
    "      for c in w + '.':\n",
    "        ix = CTOI[c]\n",
    "        xs.append(context)\n",
    "        ys.append(CTOI[c])\n",
    "        context = context[1:] + [ix]\n",
    "    xs = torch.tensor(xs)\n",
    "    ys = torch.tensor(ys)\n",
    "    return xs, ys\n",
    "\n",
    "  def init_network(self):\n",
    "    g = torch.Generator().manual_seed(2147483647)\n",
    "    self.C = torch.randn((NUM_CHARS, self.embed_dim), generator=g, requires_grad=True)\n",
    "    # C[xs].shape = (num_data_pairs, context_len, embed_dim)\n",
    "    self.W = torch.randn(((self.n-1)*self.embed_dim, NUM_CHARS), generator=g, requires_grad=True)\n",
    "    self.params = [self.C, self.W]\n",
    "    for p in self.params:\n",
    "      p.requires_grad = True\n",
    "\n",
    "\n",
    "  def train_network(self, num_iters: int, lr: float):\n",
    "    losses = []\n",
    "    for k in range(num_iters):\n",
    "      # NN forward pass\n",
    "      xenc = self.C[self.xs]\n",
    "      xenc = xenc.view(-1, (self.n-1)*self.embed_dim)\n",
    "      logits = xenc @ self.W             # log counts -> only thing that will change in Transformers\n",
    "      counts = logits.exp()              # equivalent to counts\n",
    "      P = counts / counts.sum(dim=1, keepdims=True)\n",
    "      # last 2 lines: softmax\n",
    "      \n",
    "      # loss: negative llh of probs corresponding to true labels\n",
    "      loss = F.cross_entropy(logits, self.ys) + self.rc*(self.W**2).mean()\n",
    "      # loss = -P[torch.arange(self.ys.nelement()), self.ys].log().mean() + self.rc*(self.W**2).mean()\n",
    "      \n",
    "      ## NN backward pass\n",
    "      for p in self.params:\n",
    "        p.grad = None       # set grad to 0\n",
    "      loss.backward()\n",
    "      if k%10 == 0:\n",
    "        print(f'Iter {k}, loss {loss.item()}')\n",
    "      losses.append(loss.item())\n",
    "      for p in self.params:\n",
    "        p.data += -lr*p.grad\n",
    "    plt.plot(range(len(losses)), losses)\n",
    "    plt.show()\n",
    "\n",
    "\n",
    "  def sample(self, generator: torch.Generator, num_samples: int):\n",
    "    for i in range(num_samples):\n",
    "      sample = []\n",
    "      context = [0 for _ in range(self.n-1)]\n",
    "      while True:\n",
    "        xenc = self.C[torch.tensor(context)].flatten().unsqueeze(0)\n",
    "        logits = xenc @ self.W\n",
    "        counts = logits.exp()\n",
    "        p = F.normalize(counts.float(), p=1, dim=-1)\n",
    "\n",
    "        ix = torch.multinomial(p, num_samples=1, replacement=True, generator=generator).item()\n",
    "        sample.append(ITOC[ix])\n",
    "        if ix == 0:\n",
    "          break\n",
    "        context = context[1:] + [ix]\n",
    "\n",
    "      print(''.join(sample))\n",
    "\n",
    "  def eval(self, eval_set: Union[str, Dataset] = 'eval'):\n",
    "    # average neg log likelihood\n",
    "    xs, ys = self.prepare_ds(eval_set)\n",
    "    xenc = self.C[xs]\n",
    "    xenc = xenc.view(-1, (self.n-1)*self.embed_dim)\n",
    "    logits = xenc @ self.W    \n",
    "    loss = F.cross_entropy(logits, ys) + self.rc*(self.W**2).mean()      \n",
    "    # counts = logits.exp()             \n",
    "    # P = counts / counts.sum(dim=1, keepdims=True)\n",
    "    # loss = -P[torch.arange(ys.nelement()), ys].log().mean() + self.rc*(self.W**2).mean() \n",
    "    return loss.item()\n",
    "  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 142,
   "metadata": {},
   "outputs": [],
   "source": [
    "g = torch.Generator().manual_seed(2147483647)\n",
    "data = Dataset('../../data/names.txt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 143,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(2.4743)"
      ]
     },
     "execution_count": 143,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model = NGramExplicitModel(n=2, data=data)\n",
    "model.count()\n",
    "model.eval('eval')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 149,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iter 0, loss 2.7441604137420654\n",
      "Iter 10, loss 2.736665725708008\n",
      "Iter 20, loss 2.7295589447021484\n",
      "Iter 30, loss 2.722806930541992\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[149], line 5\u001b[0m\n\u001b[0;32m      2\u001b[0m \u001b[39mfor\u001b[39;00m rc \u001b[39min\u001b[39;00m [\u001b[39m0.5\u001b[39m]:\n\u001b[0;32m      3\u001b[0m   \u001b[39m# model = NGramMLPModel(2, data, 20, rc)\u001b[39;00m\n\u001b[0;32m      4\u001b[0m   model\u001b[39m.\u001b[39mlr \u001b[39m=\u001b[39m \u001b[39m0.5\u001b[39m\n\u001b[1;32m----> 5\u001b[0m   model\u001b[39m.\u001b[39;49mtrain_network(\u001b[39m250\u001b[39;49m, model\u001b[39m.\u001b[39;49mlr)\n\u001b[0;32m      6\u001b[0m   loss \u001b[39m=\u001b[39m model\u001b[39m.\u001b[39meval(\u001b[39m'\u001b[39m\u001b[39meval\u001b[39m\u001b[39m'\u001b[39m)\n\u001b[0;32m      7\u001b[0m   \u001b[39mprint\u001b[39m(rc, loss\u001b[39m-\u001b[39mrc\u001b[39m*\u001b[39m(model\u001b[39m.\u001b[39mW\u001b[39m*\u001b[39m\u001b[39m*\u001b[39m\u001b[39m2\u001b[39m)\u001b[39m.\u001b[39mmean())\n",
      "Cell \u001b[1;32mIn[141], line 57\u001b[0m, in \u001b[0;36mNGramMLPModel.train_network\u001b[1;34m(self, num_iters, lr)\u001b[0m\n\u001b[0;32m     55\u001b[0m \u001b[39mfor\u001b[39;00m p \u001b[39min\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mparams:\n\u001b[0;32m     56\u001b[0m   p\u001b[39m.\u001b[39mgrad \u001b[39m=\u001b[39m \u001b[39mNone\u001b[39;00m       \u001b[39m# set grad to 0\u001b[39;00m\n\u001b[1;32m---> 57\u001b[0m loss\u001b[39m.\u001b[39;49mbackward()\n\u001b[0;32m     58\u001b[0m \u001b[39mif\u001b[39;00m k\u001b[39m%\u001b[39m\u001b[39m10\u001b[39m \u001b[39m==\u001b[39m \u001b[39m0\u001b[39m:\n\u001b[0;32m     59\u001b[0m   \u001b[39mprint\u001b[39m(\u001b[39mf\u001b[39m\u001b[39m'\u001b[39m\u001b[39mIter \u001b[39m\u001b[39m{\u001b[39;00mk\u001b[39m}\u001b[39;00m\u001b[39m, loss \u001b[39m\u001b[39m{\u001b[39;00mloss\u001b[39m.\u001b[39mitem()\u001b[39m}\u001b[39;00m\u001b[39m'\u001b[39m)\n",
      "File \u001b[1;32mc:\\Users\\maxsi\\miniconda3\\envs\\isp\\lib\\site-packages\\torch\\_tensor.py:488\u001b[0m, in \u001b[0;36mTensor.backward\u001b[1;34m(self, gradient, retain_graph, create_graph, inputs)\u001b[0m\n\u001b[0;32m    478\u001b[0m \u001b[39mif\u001b[39;00m has_torch_function_unary(\u001b[39mself\u001b[39m):\n\u001b[0;32m    479\u001b[0m     \u001b[39mreturn\u001b[39;00m handle_torch_function(\n\u001b[0;32m    480\u001b[0m         Tensor\u001b[39m.\u001b[39mbackward,\n\u001b[0;32m    481\u001b[0m         (\u001b[39mself\u001b[39m,),\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    486\u001b[0m         inputs\u001b[39m=\u001b[39minputs,\n\u001b[0;32m    487\u001b[0m     )\n\u001b[1;32m--> 488\u001b[0m torch\u001b[39m.\u001b[39;49mautograd\u001b[39m.\u001b[39;49mbackward(\n\u001b[0;32m    489\u001b[0m     \u001b[39mself\u001b[39;49m, gradient, retain_graph, create_graph, inputs\u001b[39m=\u001b[39;49minputs\n\u001b[0;32m    490\u001b[0m )\n",
      "File \u001b[1;32mc:\\Users\\maxsi\\miniconda3\\envs\\isp\\lib\\site-packages\\torch\\autograd\\__init__.py:197\u001b[0m, in \u001b[0;36mbackward\u001b[1;34m(tensors, grad_tensors, retain_graph, create_graph, grad_variables, inputs)\u001b[0m\n\u001b[0;32m    192\u001b[0m     retain_graph \u001b[39m=\u001b[39m create_graph\n\u001b[0;32m    194\u001b[0m \u001b[39m# The reason we repeat same the comment below is that\u001b[39;00m\n\u001b[0;32m    195\u001b[0m \u001b[39m# some Python versions print out the first line of a multi-line function\u001b[39;00m\n\u001b[0;32m    196\u001b[0m \u001b[39m# calls in the traceback and some print out the last line\u001b[39;00m\n\u001b[1;32m--> 197\u001b[0m Variable\u001b[39m.\u001b[39;49m_execution_engine\u001b[39m.\u001b[39;49mrun_backward(  \u001b[39m# Calls into the C++ engine to run the backward pass\u001b[39;49;00m\n\u001b[0;32m    198\u001b[0m     tensors, grad_tensors_, retain_graph, create_graph, inputs,\n\u001b[0;32m    199\u001b[0m     allow_unreachable\u001b[39m=\u001b[39;49m\u001b[39mTrue\u001b[39;49;00m, accumulate_grad\u001b[39m=\u001b[39;49m\u001b[39mTrue\u001b[39;49;00m)\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "%matplotlib inline\n",
    "for rc in [0.5]:\n",
    "  # model = NGramMLPModel(2, data, 20, rc)\n",
    "  model.lr = 1.0\n",
    "  model.train_network(250, model.lr)\n",
    "  loss = model.eval('eval')\n",
    "  print(rc, loss-rc*(model.W**2).mean())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 148,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "moq.\n",
      "awr.\n",
      "minaynnnyles.\n",
      "kmankairah.\n",
      "andevhizarie.\n"
     ]
    }
   ],
   "source": [
    "g = torch.Generator().manual_seed(2147483647)\n",
    "model.sample(generator=g, num_samples=5)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "isp",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "21cbe5e4507590016ddf6079d506494e7f2eacb4fe848e3412c5364edd85520f"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
