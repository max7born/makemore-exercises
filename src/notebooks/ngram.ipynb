{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import torch\n",
    "import torch.nn.functional as F\n",
    "from sklearn.model_selection import train_test_split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import string\n",
    "NUM_CHARS = 27\n",
    "CHARS = list(string.ascii_lowercase)\n",
    "CTOI = {c: i+1 for i, c in enumerate(CHARS)}\n",
    "CTOI['.'] = 0\n",
    "ITOC = {i: c for c, i in CTOI.items()}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Dataset:\n",
    "\n",
    "  def __init__(self, path: str, split: list = [0.8, 0.1, 0.1]) -> None:\n",
    "    self.words = open(path, 'r').read().splitlines()\n",
    "    self.train_set, test_eval = train_test_split(self.words, train_size=split[0])\n",
    "    self.eval_set, self.test_set = train_test_split(test_eval, train_size=split[1])\n",
    "\n",
    "data = Dataset('../../data/names.txt')\n",
    "print(len(data.train_set))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class NGramExplicitModel:\n",
    "\n",
    "  def __init__(self, n: int, data: Dataset) -> None:\n",
    "    self.n = n\n",
    "    self.data = data\n",
    "\n",
    "  def count(self):\n",
    "    self.counts = torch.zeros(tuple([NUM_CHARS for _ in range(self.n)]), dtype=torch.int32)\n",
    "    for word in self.data.train_set:\n",
    "      context = [0] * (self.n-1)\n",
    "      for c in word + '.':\n",
    "        ix = CTOI[c]\n",
    "        indices = tuple(context + [ix])\n",
    "        self.counts[indices] += 1\n",
    "        context = context[1:] + [ix]\n",
    "      \n",
    "    self.P = F.normalize((self.counts+1).float(), p=1, dim=-1)\n",
    "  \n",
    "  def sample(self, generator: torch.Generator, num_samples: int = 1):\n",
    "    for i in range(num_samples):\n",
    "      sample = []\n",
    "      context = [0 for _ in range(self.n-1)]\n",
    "      while True:\n",
    "        ix = torch.multinomial(self.P[tuple(context)], num_samples=1, replacement=True, generator=generator).item()\n",
    "        sample.append(ITOC[ix])\n",
    "        if ix == 0:\n",
    "          break\n",
    "        context = context[1:] + [ix]\n",
    "\n",
    "      print(''.join(sample))\n",
    "\n",
    "  def eval(self, mode: str = 'eval'):\n",
    "    # average neg log likelihood\n",
    "    llh, n = 0, 0\n",
    "    eval_set = self.data.eval_set if mode=='eval' else self.data.train_set if mode=='train' else self.data.test_set\n",
    "    for word in eval_set:\n",
    "      context = [0 for _ in range(self.n-1)]\n",
    "      for c in word + '.':\n",
    "        ix = CTOI[c]\n",
    "        indices = tuple(context + [ix])\n",
    "        logprob = torch.log(self.P[indices])\n",
    "        llh += logprob\n",
    "        n += 1\n",
    "    return -llh/n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class NGramMLPModel:\n",
    "\n",
    "  def __init__(self, n: int, data: Dataset, embed_dim: int) -> None:\n",
    "    self.n = n\n",
    "    self.data = data\n",
    "    self.embed_dim = embed_dim\n",
    "    self.prepare_ds()\n",
    "    self.init_network()\n",
    "\n",
    "  def prepare_ds(self):\n",
    "    # Create train set\n",
    "    self.xs, self.ys = [], []\n",
    "    for w in self.data.train_set:\n",
    "      context = [0] * (self.n-1) \n",
    "      for c in w + '.':\n",
    "        ix = CTOI[c]\n",
    "        self.xs.append(context)\n",
    "        self.ys.append(CTOI[c])\n",
    "        context = context[1:] + [ix]\n",
    "    self.xs = torch.tensor(self.xs)\n",
    "    self.ys = torch.tensor(self.ys)\n",
    "\n",
    "  def init_network(self):\n",
    "    g = torch.Generator().manual_seed(2147483647)\n",
    "    self.C = torch.randn((NUM_CHARS, self.embed_dim), generator=g, requires_grad=True)\n",
    "    # C[xs].shape = (num_data_pairs, context_len, embed_dim)\n",
    "    self.W = torch.randn(((self.n-1)*self.embed_dim, NUM_CHARS), generator=g, requires_grad=True)\n",
    "    self.params = [self.C, self.W]\n",
    "    for p in self.params:\n",
    "      p.requires_grad = True\n",
    "\n",
    "\n",
    "  def train_network(self, num_iters: int, lr: float):\n",
    "    for k in range(num_iters):\n",
    "      # NN forward pass\n",
    "      xenc = self.C[self.xs]\n",
    "      xenc = xenc.view(-1, (self.n-1)*self.embed_dim)\n",
    "      logits = xenc @ self.W             # log counts -> only thing that will change in Transformers\n",
    "      counts = logits.exp()              # equivalent to counts\n",
    "      P = counts / counts.sum(dim=1, keepdims=True)\n",
    "      # last 2 lines: softmax\n",
    "      \n",
    "      # loss: negative llh of probs corresponding to true labels\n",
    "      loss = -P[torch.arange(self.ys.nelement()), self.ys].log().mean() + 0.01*(self.W**2).mean()\n",
    "      \n",
    "      ## NN backward pass\n",
    "      self.W.grad = None       # set grad to 0\n",
    "      loss.backward()\n",
    "      if k%10 == 0:\n",
    "        print(f'Iter {k}, loss {loss.item()}')\n",
    "\n",
    "      self.W.data += -lr*self.W.grad\n",
    "\n",
    "\n",
    "  def sample(self, generator: torch.Generator, num_samples: int):\n",
    "    for i in range(num_samples):\n",
    "      sample = []\n",
    "      context = [0 for _ in range(self.n-1)]\n",
    "      while True:\n",
    "        xenc = self.C[torch.tensor(context)].flatten().unsqueeze(0)\n",
    "        logits = xenc @ self.W\n",
    "        counts = logits.exp()\n",
    "        p = F.normalize(counts.float(), p=1, dim=-1)\n",
    "\n",
    "        ix = torch.multinomial(p, num_samples=1, replacement=True, generator=generator).item()\n",
    "        sample.append(ITOC[ix])\n",
    "        if ix == 0:\n",
    "          break\n",
    "        context = context[1:] + [ix]\n",
    "\n",
    "      print(''.join(sample))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "g = torch.Generator().manual_seed(2147483647)\n",
    "data = Dataset('../../data/names.txt')\n",
    "model = NGramExplicitModel(n=2, data=data)\n",
    "model.count()\n",
    "model.eval(mode='train')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# model = NGramMLPModel(2, data, 20)\n",
    "model.train_network(250, 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "g = torch.Generator().manual_seed(2147483647)\n",
    "model.sample(generator=g, num_samples=5)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "isp",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "21cbe5e4507590016ddf6079d506494e7f2eacb4fe848e3412c5364edd85520f"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
