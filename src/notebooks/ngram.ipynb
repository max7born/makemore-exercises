{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import torch\n",
    "import torch.nn.functional as F\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "from typing import Union"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import string\n",
    "NUM_CHARS = 27\n",
    "CHARS = list(string.ascii_lowercase)\n",
    "CTOI = {c: i+1 for i, c in enumerate(CHARS)}\n",
    "CTOI['.'] = 0\n",
    "ITOC = {i: c for c, i in CTOI.items()}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Dataset:\n",
    "\n",
    "  def __init__(self, path: str, split: list = [0.8, 0.1, 0.1]) -> None:\n",
    "    self.words = open(path, 'r').read().splitlines()\n",
    "    self.train_set, test_eval = train_test_split(self.words, train_size=split[0])\n",
    "    self.eval_set, self.test_set = train_test_split(test_eval, train_size=split[1])\n",
    "\n",
    "data = Dataset('../../data/names.txt')\n",
    "print(len(data.train_set))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class NGramExplicitModel:\n",
    "\n",
    "  def __init__(self, n: int, data: Dataset) -> None:\n",
    "    self.n = n\n",
    "    self.data = data\n",
    "\n",
    "  def count(self):\n",
    "    self.counts = torch.zeros(tuple([NUM_CHARS for _ in range(self.n)]), dtype=torch.int32)\n",
    "    for word in self.data.train_set:\n",
    "      context = [0] * (self.n-1)\n",
    "      for c in word + '.':\n",
    "        ix = CTOI[c]\n",
    "        indices = tuple(context + [ix])\n",
    "        self.counts[indices] += 1\n",
    "        context = context[1:] + [ix]\n",
    "      \n",
    "    self.P = F.normalize((self.counts+1).float(), p=1, dim=-1)\n",
    "  \n",
    "  def sample(self, generator: torch.Generator, num_samples: int = 1):\n",
    "    for i in range(num_samples):\n",
    "      sample = []\n",
    "      context = [0 for _ in range(self.n-1)]\n",
    "      while True:\n",
    "        ix = torch.multinomial(self.P[tuple(context)], num_samples=1, replacement=True, generator=generator).item()\n",
    "        sample.append(ITOC[ix])\n",
    "        if ix == 0:\n",
    "          break\n",
    "        context = context[1:] + [ix]\n",
    "\n",
    "      print(''.join(sample))\n",
    "\n",
    "  def eval(self, eval_set: Union[str, Dataset] = 'eval'):\n",
    "    # average neg log likelihood\n",
    "    llh, n = 0, 0\n",
    "    if type(eval_set) is str:\n",
    "      eval_set = self.data.eval_set if eval_set=='eval' else self.data.train_set if eval_set=='train' else self.data.test_set\n",
    "    else:\n",
    "      eval_set = eval_set.eval_set\n",
    "    for word in eval_set:\n",
    "      context = [0 for _ in range(self.n-1)]\n",
    "      for c in word + '.':\n",
    "        ix = CTOI[c]\n",
    "        indices = tuple(context + [ix])\n",
    "        logprob = torch.log(self.P[indices])\n",
    "        llh += logprob\n",
    "        n += 1\n",
    "        context = context[1:] + [ix]\n",
    "    return -llh/n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class NGramMLPModel:\n",
    "\n",
    "  def __init__(self, n: int, data: Dataset, embed_dim: int, rc: float, hidden_layer_size=100) -> None:\n",
    "    self.n = n\n",
    "    self.data = data\n",
    "    self.embed_dim = embed_dim\n",
    "    self.rc = rc          # regularization coefficient\n",
    "    self.xs, self.ys = self.prepare_ds()\n",
    "    self.init_network(hidden_layer_size)\n",
    "\n",
    "  def prepare_ds(self, prepare_set: Union[str, Dataset] = 'train'):\n",
    "    if type(prepare_set) is str:\n",
    "      prepare_set = self.data.eval_set if prepare_set=='eval' else self.data.train_set if prepare_set=='train' else self.data.test_set\n",
    "    else: \n",
    "      prepare_set = prepare_set.train_set\n",
    "    # Create train set\n",
    "    xs, ys = [], []\n",
    "    for w in prepare_set:\n",
    "      context = [0] * (self.n-1) \n",
    "      for c in w + '.':\n",
    "        ix = CTOI[c]\n",
    "        xs.append(context)\n",
    "        ys.append(CTOI[c])\n",
    "        context = context[1:] + [ix]\n",
    "    xs = torch.tensor(xs)\n",
    "    ys = torch.tensor(ys)\n",
    "    return xs, ys\n",
    "\n",
    "  def init_network(self, hidden_layer_size: int):\n",
    "    g = torch.Generator().manual_seed(2147483647)\n",
    "    self.C = torch.randn((NUM_CHARS, self.embed_dim), generator=g, requires_grad=True)\n",
    "    # C[xs].shape = (num_data_pairs, context_len, embed_dim)\n",
    "    self.W1 = torch.randn(((self.n-1)*self.embed_dim, hidden_layer_size), generator=g, requires_grad=True)\n",
    "    self.b1 = torch.randn(hidden_layer_size)\n",
    "    self.W2 = torch.randn((hidden_layer_size, NUM_CHARS), generator=g, requires_grad=True)\n",
    "    self.b2 = torch.randn(NUM_CHARS)\n",
    "    self.params = [self.C, self.W1, self.b1, self.W2, self.b2]\n",
    "    for p in self.params:\n",
    "      p.requires_grad = True\n",
    "\n",
    "\n",
    "  def train_network(self, num_iters: int, lr: float):\n",
    "    losses = []\n",
    "    for k in range(num_iters):\n",
    "      # NN forward pass\n",
    "      xenc = self.C[self.xs]\n",
    "      xenc = xenc.view(-1, (self.n-1)*self.embed_dim)\n",
    "      h = torch.tanh(xenc @ self.W1 + self.b1)             # log counts -> only thing that will change in Transformers\n",
    "      logits = h @ self.W2 + self.b2\n",
    "      # counts = logits.exp()              # equivalent to counts\n",
    "      # P = counts / counts.sum(dim=1, keepdims=True)\n",
    "      # last 2 lines: softmax\n",
    "      \n",
    "      # loss: negative llh of probs corresponding to true labels\n",
    "      loss = F.cross_entropy(logits, self.ys) # + self.rc*(self.W**2).mean()\n",
    "      # loss = -P[torch.arange(self.ys.nelement()), self.ys].log().mean() + self.rc*(self.W**2).mean()\n",
    "      \n",
    "      ## NN backward pass\n",
    "      for p in self.params:\n",
    "        p.grad = None       # set grad to 0\n",
    "      loss.backward()\n",
    "      if k%10 == 0:\n",
    "        print(f'Iter {k}, loss {loss.item()}')\n",
    "      losses.append(loss.item())\n",
    "      for p in self.params:\n",
    "        p.data += -lr*p.grad\n",
    "    plt.plot(range(len(losses)), losses)\n",
    "    plt.ylim(bottom=0)\n",
    "    plt.xlabel('Training epoch'); plt.ylabel('Loss')\n",
    "    plt.show()\n",
    "\n",
    "\n",
    "  def sample(self, generator: torch.Generator, num_samples: int):\n",
    "    for i in range(num_samples):\n",
    "      sample = []\n",
    "      context = [0 for _ in range(self.n-1)]\n",
    "      while True:\n",
    "        xenc = self.C[torch.tensor(context)].flatten().unsqueeze(0)\n",
    "        logits = xenc @ self.W\n",
    "        counts = logits.exp()\n",
    "        p = F.normalize(counts.float(), p=1, dim=-1)\n",
    "\n",
    "        ix = torch.multinomial(p, num_samples=1, replacement=True, generator=generator).item()\n",
    "        sample.append(ITOC[ix])\n",
    "        if ix == 0:\n",
    "          break\n",
    "        context = context[1:] + [ix]\n",
    "\n",
    "      print(''.join(sample))\n",
    "\n",
    "  def eval(self, eval_set: Union[str, Dataset] = 'eval'):\n",
    "    # average neg log likelihood\n",
    "    xs, ys = self.prepare_ds(eval_set)\n",
    "    xenc = self.C[xs]\n",
    "    xenc = xenc.view(-1, (self.n-1)*self.embed_dim)\n",
    "    h = torch.tanh(xenc @ self.W1 + self.b1)\n",
    "    logits = h @ self.W2 + self.b2\n",
    "    loss = F.cross_entropy(logits, ys) # + self.rc*(self.W1**2).mean()      \n",
    "    # counts = logits.exp()             \n",
    "    # P = counts / counts.sum(dim=1, keepdims=True)\n",
    "    # loss = -P[torch.arange(ys.nelement()), ys].log().mean() + self.rc*(self.W**2).mean() \n",
    "    return loss.item()\n",
    "  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "g = torch.Generator().manual_seed(2147483647)\n",
    "data = Dataset('../../data/names.txt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = NGramExplicitModel(n=2, data=data)\n",
    "model.count()\n",
    "model.eval('eval')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "for rc in [0.5]:\n",
    "  model = NGramMLPModel(2, data, 20, rc)\n",
    "  model.lr = 1.0\n",
    "  model.train_network(100, model.lr)\n",
    "  loss = model.eval('eval')\n",
    "  # print(rc, loss-rc*(model.W**2).mean())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "g = torch.Generator().manual_seed(2147483647)\n",
    "model.sample(generator=g, num_samples=5)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "isp",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "21cbe5e4507590016ddf6079d506494e7f2eacb4fe848e3412c5364edd85520f"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
